# Introduction to 2048 Program Developed at CGI Lab

This page introduces the design of the 2048 program, developed in [Computer Games and Intelligence (CGI) Lab][cgi_lab], led by Professor [I-Chen Wu][icwu], at [Department of Computer Science][nctu_cs], National Yang Ming Chiao Tung University, Taiwan; and also acknowledges the contributors who are/were the members of the CGI lab.

- [Design of 2048 Program (2014-2015)](#Design-of-2048-Program-2014-2015)
- [Design of 2048 Program (2016-2018)](#Design-of-2048-Program-2016-2018)
- [Design of 2048 Program (2019-)](#Design-of-2048-Program-2019-)
- [Design of 2048 Framework for Educational Purposes (2017-)](#Design-of-2048-Framework-for-Educational-Purposes-2017-)

---

## Design of 2048 Program (2014-2015)

Inspired by the 2048 program based on Temporal Difference (TD) Learning and N-Tuple Network proposed by [Szubert and Jaśkowski][ref_td] on IEEE CIG 2014, we developed the first version of our 2048 program with an improved TD variant: Multi-Stage Temporal Difference (MS-TD) Learning with TD(λ).

Source codes, statstics of performance, and more implementation details can be found on [CGI Lab's GitHub](https://github.com/CGI-LAB/Taiwan_Bot_Tournament_2048) and [Yeh's GitHub](https://github.com/tnmichael309/2048AI).

### Authors

<table>
<tbody>
<tr>
<td>Kun-Hao Yeh (葉騉豪)</td>
<td>2014-2015</td>
<td>Main author for the 2048 program, based on MS-TD Learning and Expectimax Search.</td>
</tr>
<tr>
<td>Chao-Chin Liang (梁朝欽)</td>
<td>2014</td>
<td>Co-author for the 2048 program.</td>
</tr>
<tr>
<td>Chia-Chuan Chang (張家銓)</td>
<td>2014</td>
<td>Co-author for the 2048 program, helped improve 2048 and applied to Threes.</td>
</tr>
<tr>
<td>I-Chen Wu (吳毅成)</td>
<td>2014-2015</td>
<td>Supervised the whole project.</td>
</tr>
</tbody>
</table>

### Achievements

- The first 2048 AI program reached 65536-tile ([link][ref_mstd]). <!-- broken link: See the [replay](http://2048.aigames.nctu.edu.tw/2048/replay.php)-->

### Features

- Propose to improve program strength with MS-TD Learning with TD(λ) ([link][ref_mstd]).
- Use N-Tuple Networks as the value function.
- Use Expectimax Search to improve the testing performance.
- The single-threaded program ran fast, about 500 moves per second.

### Competitions

<table>
<tbody>
<tr>
<th>Year</th>
<th>Competitions</th>
<th>Place</th>
</tr>
<tr>
<td>2015</td>
<td>Computer Olympiad</td>
<td>1st place (Gold)</td>
</tr>
<tr>
<td>2014</td>
<td>Taiwan 2048 Bot Contest</td>
<td>2nd place (Silver)</td>
</tr>
</tbody>
</table>

### Publications

- I-Chen Wu, Kun-Hao Yeh, Chao-Chin Liang, Chia-Chuan Chang, and Han Chiang, "Multi-stage Temporal Difference Learning for 2048", the 2014 Conference on Technologies and Applications of Artificial Intelligence (TAAI 2014), Taipei, Taiwan, Novemeber 2014. (Best Paper Award)
- Kun-Hao Yeh, Chao-Chin Liang, Kuang-Che Wu, and I-Chen Wu, 2048-Bot Tournament in Taiwan, ICGA Journal (SCI), Vol. 37(3), September 2014.

---

## Design of 2048 Program (2016-2018)

As of 2016, to handle the increasing number of total training episodes, we improved MS-TD Learning with lock-free parallel training to speed up the training speed. To reduce the use of resources of Exceptimax Search, we added a technique to dynamically select search depth. Also, we integrated Temporal Coherence (TC) Learning proposed by [Jaśkowski][ref_tc] into our program. The source code and more details can be found on [Guei's GitHub](https://github.com/moporgic/TDL2048).

On the other hand, we attempted to replace N-Tuple Network with Deep Convolutional Neural Network (DCNN) in 2048. The source code and more implementation details can be found on [Huang's GitHub](https://github.com/jinbo-huang/2048_DRL).

### Authors

<table>
<tbody>
<tr>
<td>Hung Guei (桂浤)</td>
<td>2016-current</td>
<td>Main author for the 2048 program, improved the program performance with parallel training, dynamic search depth, TC Learning, etc.</td>
</tr>
<tr>
<td>Kun-Hao Yeh (葉騉豪)</td>
<td>2016</td>
<td>Co-author for the 2048 program, assisted on the MS-TD Learning.</td>
</tr>
<tr>
<td>Jin-Bo Huang (黃勁博)</td>
<td>2016</td>
<td>Co-author for the 2048 program, assisted on DCNN.</td>
</tr>
<tr>
<td>I-Chen Wu (吳毅成)</td>
<td>2016-current</td>
<td>Supervised the whole project.</td>
</tr>
</tbody>
</table>

### Features

- Improve the training speed of MS-TD Learning with parallel training.
- Improve the efficiency of the Expectimax Search by using dynamic search depth.
- Use N-Tuple Networks suggested by [Yeh et al.][ref_mstd] as the value function.
- Implement TC Learning to achieve fast convergence.
- Attempt to use DCNN as the value function.

### Competitions

<table>
<tbody>
<tr>
<th>Year</th>
<th>Competitions</th>
<th>Place</th>
</tr>
<tr>
<td>2016</td>
<td>Computer Olympiad</td>
<td>1st place (Gold)</td>
</tr>
</tbody>
</table>

### Publications

- Kun-Hao Yeh, I-Chen Wu, Chu-Hsuan Hsueh, Chia-Chuan Chang, Chao-Chin Liang, and Han Chiang, Multi-Stage Temporal Difference Learning for 2048-like Games, IEEE Transactions on Computational Intelligence and AI in Games, Vol. 10, No. 4, pp 369-380, December 2017.
- Hung Guei, Tinghan Wei, Jin-Bo Huang, I-Chen Wu, "An Empirical Study on Applying Deep Reinforcement Learning to the Game 2048", the Workshop Neural Networks in Games in the International Conference on Computers and Games (CG 2016), Leiden, the Netherlands, June, 2016.

---

## Design of 2048 Program (2019-)

As of 2019, we started investigating the exploration of learning in 2048, and proposed to apply Optimistic Initialization (OI) to TD Learning, namely Optimistic TD (OTD) Learning. To improve the program strength, we used more complex networks proposed by [Matsuzaki][ref_ntn]. To further improve the program speed, we refactored the program with more efficient designs, including network feature extraction, bitboard operations, search algorithms, etc. With improvements above, we achieved a new state-of-the-art results, namely an average score of 625,377 and a rate of 72% reaching 32768-tiles. Its statistics of performance are as follows.

|Search Depth|  Ave. Score|8192 [%]|16384 [%]|32768 [%]|  # Tested Games|
|:-----------|:------:|:------:|:-------:|:-------:|--------:|
|1-ply       | 412,785|  97.24%|   85.39%|   30.16%|1,000,000|
|2-ply       | 513,301|  99.17%|   94.40%|   48.92%|  100,000|
|3-ply       | 563,316|  99.63%|   96.88%|   57.90%|   10,000|
|4-ply       | 586,720|  99.60%|   98.60%|   62.00%|    1,000|
|5-ply       | 608,679|  99.80%|   97.80%|   67.40%|      100|
|6-ply       | 625,377|  99.80%|   98.80%|   72.00%|      100|


Source codes and more implementation details can be found on [Guei's GitHub](https://github.com/moporgic/TDL2048).

### Achievements

- The state-of-the-art 2048 AI program as of September 2021, namely an average score of 625,377 and a rate of 72% reaching 32768-tiles ([link][ref_otd]). 
- For sufficiently large tests, 65536-tiles are reached at a rate of 0.02% ([link][ref_otd]).

### Authors

<table>
<tbody>
<tr>
<td>Hung Guei (桂浤)</td>
<td>2019-current</td>
<td>Main author for the 2048 program, based on Multi-Stage Optimistic TD Learning, Expectimax Tree Search, and other improvements.</td>
</tr>
<tr>
<td>I-Chen Wu (吳毅成)</td>
<td>2019-current</td>
<td>Supervised the whole project.</td>
</tr>
</tbody>
</table>

### Features

- Propose to improve exploration of training by Optimistic Initialization ([link][ref_otd]).
- Use N-Tuple Networks suggested by [Matsuzaki][ref_ntn] as the value function.
- Expectimax Tree Search with transposition table and tile-downgrading technique.
- Effective bitboard move generation and tile generation.
- Fast single-threaded evaluation: More than 1,500,000 moves and 4,000 moves per second when using 1-ply and 3-ply expectimax search, respectively.
- Fast multi-threaded training: More than 10,000,000 states per second when using 20 cores.

### Publications

- Hung Guei, Lung-Pin Chen, I-Chen Wu, "Optimistic Temporal Difference Learning for 2048", accepted by the IEEE Transactions on Games, 2021.

---

## Design of 2048 Framework for Educational Purposes (2017-)

Based on our experiences in developing 2048 programs, we noticed that 2048 is highly suitable for educational purposes, especially for teaching Reinforcement Learning (RL) and simple AI algorithms. To help students focus on algorithms, we started developing a framework for educational purposes. The framework and sample course designs are available at [2048-Framework](https://github.com/moporgic/2048-Framework) (C++) and [2048-Framework-Python](https://github.com/moporgic/2048-Framework-Python) (Python 3).

### Authors

<table>
<tbody>
<tr>
<td>Hung Guei (桂浤)</td>
<td>2017-current</td>
<td>Main author for the 2048 framework.</td>
</tr>
<tr>
<td>I-Chen Wu (吳毅成)</td>
<td>2017-current</td>
<td>Supervised the whole project.</td>
</tr>
</tbody>
</table>

### Features

- Support the development of TD Learning and its variants, search algorithms, and other machine learning algorithms. 
- Highly customizable environment, can be easily changed to other 2048-like games, such as Threes! and 2584 Fibonacci.
- In response to different project needs, both C++ and Python implementations are provided.

### Publications

- Hung Guei, Tinghan Wei, I-Chen Wu, "2048-like games for teaching reinforcement learning", ICGA Journal, Vol. 42, No. 1, January 2020.
- Hung Guei, Tinghan Wei, I-Chen Wu, "Teaching Reinforcement Learning and Computer Games with 2048-Like Games", The 33th Annual Conference of The Japanese Society for Artificial Intelligence (JSAI 2019), Niigata, Japan, June 2019.
- Hung Guei, Tinghan Wei, and I-Chen Wu. Using 2048-like Games as a Pedagogical Tool for Reinforcement Learning. International Conference on Computers and Games (CG2018), New Taipei City, Taiwan, July 2018.

<!---------------------------------------------------------------------->

[icwu]: ..
[cgi_lab]: https://cgilab.nctu.edu.tw/
[nctu_cs]: https://www.cs.nycu.edu.tw/
[ref_td]: http://www.cs.put.poznan.pl/wjaskowski/pub/papers/Szubert2014_2048.pdf
[ref_mstd]: https://doi.org/10.1109/TCIAIG.2016.2593710
[ref_tc]: https://doi.org/10.1109/TCIAIG.2017.2651887
[ref_otd]: https://doi.org/10.1109/TG.2021.3109887
[ref_ntn]: https://doi.org/10.1109/TAAI.2016.7880154